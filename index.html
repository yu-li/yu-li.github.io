<!DOCTYPE html
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">

<head>
    <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="icon" type="x-icon" href="img/idea.jpg" />
    <title>Yu Li | IDEA</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="style.css" type="text/css" media="screen">
</head>

<body>
    <div id="wrapperhead">
        <div id="wrapperindex">
            <div id="container">
                <div id="about">
                    <div id="header">
                        <h2>Yu LI (李昱)</h2>
                        <!-- <div class="logom"><img src="img/lyLogo.png" alt="Yu's LOGO" width="180" height="60"></div> -->
                    </div>
                    <h4>Principal Researcher</h4>
                    <h4><a href="https://idea.edu.cn/en">International Digital Economy Academy (IDEA)</a></h4>
                    <h4>Guangdong-HongKong-Macau Greater Bay Area</h4>
                    <h4>Shenzhen, China</h4>
                    <br>
                    <h4>&nbsp; &nbsp; <i class="fa fa-envelope"></i> &nbsp; liyu[at]idea.edu.cn</h4>
                    <br>
                    <div align="left">
                        &nbsp;&nbsp;<a href="https://scholar.google.com/citations?user=j9lwU7kAAAAJ&hl=en"><i style="font-size:26px"
                                class="ai ai-google-scholar"></i>
                            Google Scholar
                        </a>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/yu-li"><i style="font-size:26px"
                                class="fa fa-github"></i>
                            Github
                        </a>
                        <!-- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://twitter.com/ianlee822"><i style="font-size:26px"
                                class="fa fa-twitter"></i>
                            Twitter
                        </a> -->
                    </div>
                </div>
                <div id="photo"><img src="img/profile.gif" alt="Yu's Profile Picture" width="224" height="224"></div>

            </div>

            <div id="container">
                <div id="aboutlong">
                    <p>I am a Principal Researcher at International Digital Economy Academy (IDEA),
                        where I lead IDEA's <a href="https://www.idea.edu.cn/research/idea-vistring-lab.html">Vistring Lab</a> and focus on video content generation and editing.
                        Prior to joining IDEA, I worked at
                        <a href="https://arc.tencent.com/en/index">Tencent ARC Lab</a>,
                        <a href="https://www.blackmagicdesign.com/products/davinciresolve/">Blackmagic Design</a>, and
                        <a href="http://adsc.illinois.edu/">Advanced Digital Sciences Center</a>.
                        I hold a PhD degree from National University of Singapore, advised by Prof.
                        <a href="https://www.eecs.yorku.ca/~mbrown/">Michael S Brown</a>.
                        Additionally, I am a research affiliate with the CSL Lab, University of Illinois at
                        Urbana-Champaign and an adjunct professor at the South China University of Technology.
                        My research interests lie at the intersection of computer vision and computer graphics, with a particular focus on computational photography.
                        I have served as an Area Chair for CVPR & NeurIPS, and a Guest Editor of IJCV.
                        <br>
                        <font color="orange"> We are hiring interns at IDEA (base: Shenzhen). Feel free to contact me if you
                            are interested. <a href="https://zhuanlan.zhihu.com/p/582929545"> [details] </a></font>
                    </p><br>
                </div>
            </div>
        </div>


        <div id="container">
            <div id="mainbody">

                <h3>Recent Services: </h3>
                <ul>
                    <li>
                        Area Chair: CVPR 2023, NeurIPS 2023, CVPR 2024
                    </li>

                    <li>
                        Workshop (Co)organizer: ICCV 2017/ 2019/ 2021 PBDL Workshop
                    </li>

                    <li>
                        Guest Editor: IJCV Special Issue on Physics Based Vision meets Deep Learning
                    </li>
                </ul>
                <br>

                <h3>News: </h3>
                    <li>
                        <font color="orange"><b>[Jul. 2023]</b> Our <a href="https://github.com/IDEA-Research/DWPose"> DWPose </a> for
                        whole-body human pose estimation is the Top 1 on COCO-WholeBody Benchmark. It becomes a better alternative to
                        OpenPose.</font>
                    </li>
                    <li>
                        <font color="orange"><b>[Jul. 2023]</b> Five papers accepted by ICCV 2023 </font>
                    </li>
                    <li>
                        <font color="orange"><b>[Jun. 2023]</b> Serving as Area Chair of <a href="https://cvpr2024.thecvf.com/">CVPR 2024</a>
                        </font>
                    </li>
                    <li>
                        <font color="orange"><b>[Mar. 2023]</b> Serving as Area Chair of <a href="https://nips.cc//">NeurIPS 2023</a>  </font>
                    </li>
                <br>

                <!-- <h3>Highlights: </h3>
                <li>
                    Our <a href="https://github.com/IDEA-Research/DWPose"> Our DWPose </a> for
                        whole-body human pose estimation is the Top 1 on COCO-WholeBody Benchmark. It becomes a better alternative to
                        OpenPose. </font>
                </li> -->


                <a name="publication">&nbsp; </a>
                <h3>Selected Publications:</h3>
                <table border=0 cellpadding=0 width=100% cellspacing="10"
                    style="line-height:18pt;  border-spacing: 60 6px;">

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img
                                    src="paper/li_stem.gif" height=140
                                    width=256> </div>
                        </td>
                        <td>
                            <p> Maomao Li, <b>Yu Li</b>, Tianyu Yang, Yunfei Liu, Dongxu Yue, Zhihui Lin, Dong Xu<br>
                                <b><em>A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing
                                    </em></b><br>
                                Arxiv <br>
                                <!-- <a href="https://github.com/IDEA-Research/DWPose">
                                    <img alt="GitHub stars" style="vertical-align: middle"
                                        src="https://img.shields.io/github/stars/IDEA-Research/DWPose?style=social">
                                </a> -->
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/yang_dwpose2.gif" height=140 width=256> </div>
                        </td>
                        <td>
                            <p> Zhendong Yang, Ailing Zeng, Chun Yuan, <b>Yu Li</b> <br>
                                <b><em>DWPose: Effective Whole-body Pose Estimation with Two-stages Distillation
                                    </em></b><br>
                                ICCV-W <br>
                                <a href="https://github.com/IDEA-Research/DWPose">
                                <img alt="GitHub stars" style="vertical-align: middle"
                                    src="https://img.shields.io/github/stars/IDEA-Research/DWPose?style=social">
                                </a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/yuan_iccv2023.gif" height=180 width=256> </div>
                        </td>
                        <td>
                            <p> Ziyang Yuan, Yiming Zhu, <b>Yu Li</b>, Hongyu Liu, Chun Yuan<br>
                                <b><em>Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding </em></b><br>
                                ICCV 2023 <br>
                                <a href="https://github.com/jiangyzy/GOAE">
                                    <img alt="GitHub stars" style="vertical-align: middle"
                                        src="https://img.shields.io/github/stars/jiangyzy/GOAE?style=social">
                                </a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/lin_cvpr2023.gif" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, <b>Yu Li</b> <br>
                                <b><em>OSX: One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer </em></b><br>
                                CVPR 2023 (<i><font color="orange">Top 1 on AGORA SMPL-X Benchmark</font>
                                </i>) <br>
                                <a href="https://osx-ubody.github.io/">
                                <img alt="GitHub stars" style="vertical-align: middle"
                                    src="https://img.shields.io/github/stars/IDEA-Research/OSX?style=social">
                                </a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/weng_eccv2022.jpg" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Shuchen Weng, Jimeng Sun, <b>Yu Li</b>, Si Li, Boxin Shi<br>
                                <b><em>CT^2: Colorization Transformer via Color Tokens </em></b><br>
                                ECCV 2022 (<i>
                                    <font color="orange">oral</font>
                                </i>) <br>
                                <a href="https://github.com/shuchenweng/CT2"><img alt="GitHub stars" style="vertical-align: middle"
                                    src="https://img.shields.io/github/stars/shuchenweng/CT2?style=social"> </a> <br>
                                <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670001.pdf">[pdf]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/chen_accv2022.gif" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Xiangguang Chen*, Ye Zhu*, <b>Yu Li</b>, Bingtao Fu, Lei Sun, Ying Shan, Shan Liu<br>
                                <b><em>Robust Human Matting via Semantic Guidance </em></b><br>
                                ACCV 2022 <br>
                                <a href="https://github.com/cxgincsu/SemanticGuidedHumanMatting"><img alt="GitHub stars" style="vertical-align: middle"
                                        src="https://img.shields.io/github/stars/cxgincsu/SemanticGuidedHumanMatting?style=social"> </a> <br>
                                <a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Chen_Robust_Human_Matting_via_Semantic_Guidance_ACCV_2022_paper.pdf">[pdf]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/li_ijcv2022.gif" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> <b>Yu Li*</b>, Ye Zhu*, Ruoteng Li, Xintao Wang, Yue Luo, Ying Shan<br>
                                <b><em>Hybrid Warping Fusion for Video Frame Interpolation </em></b><br>
                                IJCV 2022 <br>
                                <a href="https://link.springer.com/article/10.1007/s11263-022-01683-9">[link]</a>
                            </p>
                        </td>
                    </tr>

                    <!-- <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/xing_mm2022.jpg" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Yazhou Xing, <b>Yu Li</b>, Xintao Wang, Ye Zhu, Qifeng Chen<br>
                                <b><em>Composite Photograph Harmonization with Complete Background Cues </em></b><br>
                                ACM MM 2022 <br>
                                <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548031">[link]</a>
                            </p>
                        </td>
                    </tr> -->

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/yang_cvpr2022.gif" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Shusheng Yang, Xinggang Wang, <b>Yu Li</b>, Yuxin Fang, Jiemin Fang, Wenyu Liu, Xun Zhao, Ying Shan<br>
                                <b><em>TeViT: Temporally Efficient Vision Transformer for Video Instance Segmentation</em></b><br>
                                CVPR 2022 (<i>
                                    <font color="orange">oral</font>
                                </i>) <br>
                                <a href="https://github.com/hustvl/TeViT"><img alt="GitHub stars" style="vertical-align: middle"
                                        src="https://img.shields.io/github/stars/hustvl/TeViT?style=social"> </a> <br>
                                <a href="https://arxiv.org/pdf/2204.08412.pdf">[pdf]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/fang_iccv2021.gif" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Yuxin Fang*, Shusheng Yang*, Xinggang Wang, <b>Yu Li</b>, Chen
                                Fang, Ying Shan, Bin Feng, Wenyu Liu<br>
                                <b><em>QueryInst: Instances as Queries</em></b><br>
                                ICCV 2021 <br>
                                <a href="https://github.com/hustvl/QueryInst"><img alt="GitHub stars" style="vertical-align: middle"
                                        src="https://img.shields.io/github/stars/hustvl/QueryInst?style=social"> </a> <br>
                                <a href="https://arxiv.org/pdf/2105.01928.pdf">[pdf]</a>
                                <a href="https://www.youtube.com/watch?v=3Fqwvn6_oUQ">[demo]</a>
                            </p>
                        </td>
                    </tr>
                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/yang_iccv2021.gif" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Shusheng Yang*, Yuxin Fang*, Xinggang Wang, <b>Yu Li</b>, Chen
                                Fang, Ying Shan, Bin Feng, Wenyu Liu<br>
                                <b><em>CrossVIS: Crossover Learning for Fast Online Video Instance
                                        Segmentation</em></b><br>
                                ICCV 2021 <br>
                                <a href="https://github.com/hustvl/CrossVIS"><img alt="GitHub stars" style="vertical-align: middle"
                                    src="https://img.shields.io/github/stars/hustvl/CrossVIS?style=social"> </a> <br>
                                <a href="https://arxiv.org/pdf/2104.05970.pdf">[pdf]</a>
                                <a href="https://www.youtube.com/watch?v=tPvYYjTgaNs">[demo]</a>
                            </p>
                        </td>
                    </tr>
                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/wu_iccv2021.jpg" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Yanze Wu, Xintao Wang, <b>Yu Li</b>, Honglun Zhang, Xun Zhao, Ying Shan <br>
                                <b><em>Towards Vivid and Diverse Image Colorization with Generative Color
                                        Prior</em></b><br>
                                ICCV 2021 <br>
                                <a href="https://github.com/ToTheBeginning/GCP-Colorization"><img alt="GitHub stars" style="vertical-align: middle"
                                        src="https://img.shields.io/github/stars/ToTheBeginning/GCP-Colorization?style=social"> </a> <br>
                                <a href="https://arxiv.org/pdf/2108.08826.pdf">[pdf]</a>
                            </p>
                        </td>
                    </tr>
                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/lv_ijcv2021.jpg" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Feifan Lv, <b>Yu Li</b>, Feng Lu<br>
                                <b><em>Attention Guided Low-light Image Enhancement with a Large Scale Low-light
                                        Simulation
                                        Dataset</em></b><br>
                                IJCV 2021 <br>
                                <a href="https://arxiv.org/pdf/1908.00682.pdf">[pdf]</a>
                                <a href="http://www.phi-ai.org/project/AgLLNet/default.htm">[website]</a>
                                <a href="https://github.com/yu-li/AGLLNet">[code]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/wang_cvpr2021.gif" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Xintao Wang, <b>Yu Li</b>, Honglun Zhang, Ying Shan<br>
                                <b><em>GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial
                                        Prior</em></b><br>
                                CVPR 2021 <br>
                                <a href="https://github.com/TencentARC/GFPGAN"><img alt="GitHub stars" style="vertical-align: middle"
                                    src="https://img.shields.io/github/stars/TencentARC/GFPGAN?style=social"> </a> <br>
                                <a href="https://arxiv.org/pdf/2101.04061.pdf">[pdf]</a>
                                <a href="https://xinntao.github.io/projects/gfpgan">[website]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/zhang_cvpr2021.gif" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Fan Zhang, <b>Yu Li</b>, Shaodi You, Ying Fu<br>
                                <b><em>Learning Temporal Consistency for Low Light Video Enhancement from Single
                                        Images</em></b><br>
                                CVPR 2021 <br>
                                <a href="https://github.com/zkawfanx/StableLLVE"><img alt="GitHub stars" style="vertical-align: middle"
                                    src="https://img.shields.io/github/stars/zkawfanx/StableLLVE?style=social"> </a> <br>
                                <a
                                    href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_Temporal_Consistency_for_Low_Light_Video_Enhancement_From_Single_CVPR_2021_paper.pdf">[pdf]</a>
                                </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/lin_mm2020_vod.gif" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Lijian Lin, Haosheng Chen, Honglun Zhang, Jun Liang, <b>Yu Li</b>, Ying Shan, Hanzi
                                Wang<br>
                                <b><em>Dual Semantic Fusion Network for Video Object Detection</em></b><br>
                                ACM MM 2020 <br>
                                <a href="https://dl.acm.org/doi/pdf/10.1145/3394171.3413583">[link]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/li_eccv2020.gif" height=160 width=256> </div>
                        </td>
                        <td>
                            <p><b>Yu Li</b>*, Zhuoran Shen*, Ying Shan<br>
                                <b><em>Fast Video Object Segmentation using the Global Context Module</em></b><br>
                                ECCV 2020 <br>
                                <a
                                    href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550732.pdf">[pdf]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/liu_cvpr2020.jpg" height=160 width=256> </div>
                        </td>
                        <td>
                            <p> Yunfei Liu, <b>Yu Li</b>, Shaodi You, Feng Lu<br>
                                <b><em>Unsupervised Learning for Intrinsic Image Decomposition from a Single
                                        Image</em></b><br>
                                CVPR 2020 <br>
                                <a
                                    href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Unsupervised_Learning_for_Intrinsic_Image_Decomposition_From_a_Single_Image_CVPR_2020_paper.pdf">[pdf]</a>
                            </p>
                        </td>
                    </tr>

                    <!-- <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/hao_iccvw2019.jpg" height=150 width=256> </div>
                        </td>
                        <td>
                            <p> Zhixiang Hao, Shaodi You, <b>Yu Li</b>, Kunming Li, Feng Lu<br>
                                <b><em>Learning from Synthetic Photorealistic Raindrop for Single Image Raindrop
                                        Removal</em></b><br>
                                ICCV-W 2019<br>
                                <a
                                    href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/PBDL/Hao_Learning_From_Synthetic_Photorealistic_Raindrop_for_Single_Image_Raindrop_Removal_ICCVW_2019_paper.pdf">[pdf]</a>
                                <a href="https://github.com/DreamtaleCore/RaindropRmv">[data]</a>
                            </p>
                        </td>
                    </tr> -->

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/hao_3dv2018.jpg" height=150 width=256> </div>
                        </td>
                        <td>
                            <p> Zhixiang Hao, <b>Yu Li</b>, Shaodi You, Feng Lu<br>
                                <b><em>Detail Preserving Depth Estimation from a Single Image Using Attention Guided
                                        Networks</em></b><br>
                                3DV 2018 <br>
                                <a href="https://arxiv.org/pdf/1809.00646.pdf">[pdf]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/guo_pami2018.jpg" height=175 width=256> </div>
                        </td>
                        <td>
                            <p> Xiaojie Guo, <b>Yu Li</b>, Jiayi Ma, Haibin Ling<br>
                                <b><em>Mutually Guided Image Filtering</em></b><br>
                                ACM MM 2017 & T-PAMI 2018 <br>
                                <a href="https://ieeexplore.ieee.org/abstract/document/8550683">[link]</a>
                            </p>
                        </td>
                    </tr>

                    <!-- <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/li_arxiv2016.jpg" height=175 width=256> </div>
                        </td>
                        <td>
                            <p> <b>Yu Li</b>, Shaodi You, Michael S. Brown, Robby T. Tan <br>
                                <b><em>Haze Visibility Enhancement: A Survey and Quantitative Benchmarking</em></b><br>
                                CVIU 2017 <br>
                                <a href="https://arxiv.org/pdf/1607.06235.pdf">[pdf]</a>
                            </p>
                        </td>
                    </tr> -->

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/guo_tip2016.jpg" height=150 width=256> </div>
                        </td>
                        <td>
                            <p>Xiaojie Guo, <b>Yu Li</b>, Haibin Ling<br>
                                <b><em>LIME: Low-light Image Enhancement via Illumination Map Estimation</em></b><br>
                                T-IP 2016 <br>
                                <a href="https://ieeexplore.ieee.org/abstract/document/7782813">[link]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/lu_tpami2016.jpg" height=175 width=256> </div>
                        </td>
                        <td>
                            <p>Jiangbo Lu, <b>Yu Li</b>, Hongsheng Yang, Dongbo Min, Weiyong Eng, Minh N. Do<br>
                                <b><em>PatchMatch Filter: Edge-Aware Filtering Meets Randomized Search for
                                        Correspondence Field
                                        Estimation</em></b><br>
                                T-PAMI 2016 <br>
                                <a href="https://ieeexplore.ieee.org/abstract/document/7588057">[link]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/li_eccv16_fgi.jpg" width=256> </div>
                        </td>
                        <td>
                            <p><b>Yu Li</b>, Dongbo Min, Minh N. Do, Jiangbo Lu<br>
                                <b><em>Fast Guided Global Interpolation for Depth and Motion</em></b><br>
                                ECCV 2016 (<i>
                                    <font color="orange">spotlight</font>
                                </i>) <br>
                                <a
                                    href="http://publish.illinois.edu/visual-modeling-and-analytics/fast-guided-global-interpolation/">[project
                                    page]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/li_cvpr16_rain.jpg" width=256> </div>
                        </td>
                        <td>
                            <p><b>Yu Li</b>, Robby. T. Tan, Xiaojie Guo, Jiangbo Lu, Michael S. Brown<br>
                                <b><em>Rain Streak Removal Using Layer Priors</em></b><br>
                                CVPR 2016 <br>
                                <a href="paper/li_cvpr16_rain.pdf">[pdf]</a>
                                <a href="https://github.com/yu-li/LPDerain">[code]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/li_iccv15_mrf.gif" width=256> </div>
                        </td>
                        <td>
                            <p><b>Yu Li</b>, Dongbo Min, Michael S. Brown, Minh N. Do, Jiangbo Lu <br>
                                <b><em>SPM-BP: Sped-up PatchMatch Belief Propagation for Continuous MRFs</em></b><br>
                                ICCV 2015 (<i>
                                    <font color="orange">oral</font>
                                </i>)<br>
                                <a
                                    href="https://publish.illinois.edu/visual-modeling-and-analytics/efficient-inference-for-continuous-mrfs/">[project
                                    page]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/li_iccv15_haze.gif" width=256> </div>
                        </td>
                        <td>
                            <p><b>Yu Li</b>, Robby T. Tan, Michael S. Brown<br>
                                <b><em>Nighttime Haze Removal with Glow and Multiple Light Colors</em></b><br>
                                ICCV 2015 <br>
                                <a href="paper/li_iccv15_dehaze.pdf">[pdf]</a>
                                <a href="paper/li_iccv15_haze.zip">[code]</a>
                            </p>
                        </td>
                    </tr>


                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/li_eccv14_jpeg.gif" width=256> </div>
                        </td>
                        <td>
                            <p><b>Yu Li</b>, Fangfang Guo, Robby T. Tan, Michael S. Brown<br>
                                <b><em>A Contrast Enhancement Framework with JPEG Artifacts Suppression</em></b><br>
                                ECCV 2014 <br>
                                <a href="paper/li_eccv14_jpeg.pdf">[pdf]</a>
                                <a href="paper/li_eccv14_jpeg.zip">[code]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/li_cvpr14_layer.gif" width=256> </div>
                        </td>
                        <td>
                            <p><b>Yu Li</b>, Michael S. Brown<br>
                                <b><em>Single Image Layer Separation using Relative Smoothness</em></b><br>
                                CVPR 2014 (<i>
                                    <font color="orange">oral</font>
                                </i>)<br>
                                <a href="paper/li_cvpr14_layer.pdf">[pdf]</a>
                                <a href="paper/li_cvpr14_layer.zip">[code]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/li_iccv13_reflection.gif" width=256> </div>
                        </td>
                        <td>
                            <p><b>Yu Li</b>, Michael S. Brown<br>
                                <b><em>Exploiting Reflection Change for Automatic Reflection Removal</em></b><br>
                                ICCV 2013<br>
                                <a href="paper/li_iccv13_reflection.pdf">[pdf]</a>
                                <a href="paper/li_iccv13_reflection.zip">[code&data]</a>
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td height=135>
                            <div id="paperphoto"> <img src="paper/eg13_stitching.jpg" width=256> </div>
                        </td>
                        <td>
                            <p>Junhong Gao, <b>Yu Li</b>, Tat-Jun Chin, Michael S. Brown<br>
                                <b><em>Seam-Driven Image Stitching</em></b><br>
                                Eurographics (EG) 2013<br>
                                <a href="paper/eg13_stitching.pdf">[pdf]
                            </p>
                        </td>
                    </tr>

                </table>


                <table>
                    <tbody>
                        <tr>


                            <td width="320" align="center">
                                <div id="clustrmaps-widget"></div>
                                <script type="text/javascript" id="clustrmaps"
                                    src="//cdn.clustrmaps.com/map_v2.js?u=L99l&d=UbbIsouM2f1geopE6B8I6emni3EYAiLkmuIG3hahE6A"></script>
                            </td>
                        </tr>
                </table>
                <div id="footer">
                    <p>
                        <center> <i>Last updated on Oct. 2023</i> </center>
                    </p>
                </div>
            </div>
        </div>
    </div>
</body>

</html>
